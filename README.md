#### Bayes Rule
<img src="https://user-images.githubusercontent.com/31917400/34920230-5115b6b6-f967-11e7-9493-5f6662f1ce70.JPG" width="400" height="500" />
We know the Bayes rule. How does it relate to machine learning? Bayesian inference is based on using probability to represent **all forms of uncertainty**.

## Introduction
 - Frequentists' probability that doesn’t depend on one’s beliefs refers to **past events**..Do experiment and that's it.   
 - Bayesians' probability as a measure of beliefs refers to **future events**..posterior 

As Bayesians, we start with a belief, called a prior. Then we obtain some data and use it to update our belief. The outcome is called a posterior. Should we obtain even more data, the old posterior becomes a new prior and the cycle repeats.
<img src="https://user-images.githubusercontent.com/31917400/64063569-a9533100-cbed-11e9-89d6-a8cc6203b886.jpg"/>

P( θ | Data ) = P( Data | θ ) * P( θ ) / P( data )




































































































